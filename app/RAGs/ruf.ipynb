{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da4d8f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from functools import lru_cache\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "try:\n",
    "\t# Prefer the new package per LangChain 0.2.9+\n",
    "\tfrom langchain_chroma import Chroma  # type: ignore\n",
    "except Exception:  # pragma: no cover\n",
    "\t# Fallback for environments without langchain-chroma installed\n",
    "\tfrom langchain.vectorstores import Chroma  # deprecated path\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Paths (relative to this file)\n",
    "# In Jupyter, use the current working directory\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "\n",
    "QA_DB_PATH = str(BASE_DIR / \".chroma_qa\")\n",
    "CATALOG_DB_PATH = str(BASE_DIR / \".chroma_catalog\")\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _embeddings() -> OpenAIEmbeddings:\n",
    "\t# Keep model consistent with ingestion\n",
    "\treturn OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _qa_db() -> Chroma:\n",
    "\treturn Chroma(persist_directory=QA_DB_PATH, embedding_function=_embeddings())\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _catalog_db() -> Chroma:\n",
    "\treturn Chroma(persist_directory=CATALOG_DB_PATH, embedding_function=_embeddings())\n",
    "\n",
    "\n",
    "def _llm() -> ChatOpenAI:\n",
    "\t# Do not change user's model: allow override via env, fallback is safe\n",
    "\tmodel = os.getenv(\"OPENAI_MODEL\", os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4o-mini\"))\n",
    "\treturn ChatOpenAI(model=model, temperature=0.2)\n",
    "\n",
    "\n",
    "def _resolve_product(query: str) -> Tuple[Optional[str], Optional[str], float]:\n",
    "\t\"\"\"Resolve a product mention to (ASIN, Title, score) using the catalog index.\n",
    "\n",
    "\tReturns (asin, title, score). Lower score is better. If not found, asin/title are None.\n",
    "\t\"\"\"\n",
    "\tcatalog = _catalog_db()\n",
    "\tresults = catalog.similarity_search_with_score(query, k=1)\n",
    "\tif not results:\n",
    "\t\treturn None, None, 1e9\n",
    "\tdoc, score = results[0]\n",
    "\tasin = doc.metadata.get(\"ASIN\") if doc.metadata else None\n",
    "\ttitle = doc.metadata.get(\"Title\") if doc.metadata else None\n",
    "\treturn asin, title, score if score is not None else 0.0\n",
    "\n",
    "\n",
    "def _retrieve_product_context(question: str, asin: Optional[str], k: int = 4) -> List[str]:\n",
    "\t\"\"\"Retrieve QA passages, optionally filtering by ASIN.\"\"\"\n",
    "\tqa = _qa_db()\n",
    "\tif asin:\n",
    "\t\tdocs = qa.similarity_search(question, k=k, filter={\"ASIN\": asin})\n",
    "\t\tif not docs:  # fallback if filter too strict\n",
    "\t\t\tdocs = qa.similarity_search(question, k=k)\n",
    "\telse:\n",
    "\t\tdocs = qa.similarity_search(question, k=k)\n",
    "\treturn [d.page_content for d in docs]\n",
    "\n",
    "\n",
    "def _join_context(chunks: List[str], max_chars: int = 6000) -> str:\n",
    "\tbuf = []\n",
    "\ttotal = 0\n",
    "\tfor ch in chunks:\n",
    "\t\tif total + len(ch) > max_chars:\n",
    "\t\t\tbreak\n",
    "\t\tbuf.append(ch)\n",
    "\t\ttotal += len(ch)\n",
    "\treturn \"\\n\\n---\\n\\n\".join(buf)\n",
    "\n",
    "\n",
    "@tool(\"answer_product_question\", return_direct=False)\n",
    "def answer_product_question(question: str) -> str:\n",
    "\t\"\"\"Answer a user question about a product. Automatically detects the product from free text and uses product context. Input: question (string).\"\"\"\n",
    "\tasin, title, score = _resolve_product(question)\n",
    "\n",
    "\tcontext_chunks = _retrieve_product_context(question, asin)\n",
    "\tcontext_text = _join_context(context_chunks)\n",
    "\n",
    "\tsys = (\n",
    "\t\t\"You are a helpful customer support assistant. Answer using only the provided beauty product context. \"\n",
    "\t\t\"If the answer is not clearly supported by the context, say you are not sure. Keep responses concise.\"\n",
    "\t)\n",
    "\tprompt = ChatPromptTemplate.from_messages(\n",
    "\t\t[\n",
    "\t\t\t(\"system\", sys),\n",
    "\t\t\t(\n",
    "\t\t\t\t\"human\",\n",
    "\t\t\t\t(\n",
    "\t\t\t\t\t\"Product detected: ASIN={asin} Title={title} (confidence={score}).\\n\"\n",
    "\t\t\t\t\t\"Context:\\n{context}\\n\\nQuestion: {q}\"\n",
    "\t\t\t\t),\n",
    "\t\t\t),\n",
    "\t\t]\n",
    "\t)\n",
    "\tllm = _llm()\n",
    "\tmsg = prompt.format_messages(asin=asin, title=title, score=score, context=context_text, q=question)\n",
    "\tout = llm.invoke(msg)\n",
    "\treturn out.content\n",
    "\n",
    "\n",
    "def _parse_number_from_text(text: str, default: int, lo: int = 1, hi: int = 10) -> int:\n",
    "\t\"\"\"Extract an integer from text like 'top 5' or 'show 4 similar'. Falls back to default and clamps range.\"\"\"\n",
    "\tif not text:\n",
    "\t\treturn default\n",
    "\t# digits\n",
    "\tm = re.search(r\"\\b(\\d{1,2})\\b\", text)\n",
    "\tif m:\n",
    "\t\ttry:\n",
    "\t\t\tval = int(m.group(1))\n",
    "\t\t\treturn max(lo, min(hi, val))\n",
    "\t\texcept Exception:\n",
    "\t\t\tpass\n",
    "\t# simple number words 1-10\n",
    "\twords = {\n",
    "\t\t\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n",
    "\t\t\"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10,\n",
    "\t}\n",
    "\tfor w, v in words.items():\n",
    "\t\tif re.search(rf\"\\b{w}\\b\", text, flags=re.I):\n",
    "\t\t\treturn max(lo, min(hi, v))\n",
    "\treturn default\n",
    "\n",
    "\n",
    "@tool(\"recommend_products\", return_direct=False)\n",
    "def recommend_products(query: str, n: Optional[int] = None) -> str:\n",
    "\t\"\"\"Recommend N similar products for a given user query or product mention. Detects N from the query if present. Inputs: query (string), n (optional int).\"\"\"\n",
    "\tasin, title, _ = _resolve_product(query)\n",
    "\tcatalog = _catalog_db()\n",
    "\n",
    "\t# Detect N from query if not provided\n",
    "\tn_val = _parse_number_from_text(query, default=3) if n is None else n\n",
    "\tn_val = max(1, min(10, int(n_val)))\n",
    "\n",
    "\tanchor_text = title or query\n",
    "\t# Fetch a few extra to allow filtering out the same product\n",
    "\tresults = catalog.similarity_search_with_score(anchor_text, k=max(n_val + 2, 5))\n",
    "\n",
    "\titems = []\n",
    "\tseen = set([asin]) if asin else set()\n",
    "\tfor doc, score in results:\n",
    "\t\tcand_asin = (doc.metadata or {}).get(\"ASIN\")\n",
    "\t\tcand_title = (doc.metadata or {}).get(\"Title\")\n",
    "\t\tif not cand_asin or cand_asin in seen:\n",
    "\t\t\tcontinue\n",
    "\t\tseen.add(cand_asin)\n",
    "\t\titems.append((cand_title or \"Unknown\", cand_asin, score))\n",
    "\t\tif len(items) >= n_val:\n",
    "\t\t\tbreak\n",
    "\n",
    "\tif not items:\n",
    "\t\treturn \"No similar products found with enough confidence.\"\n",
    "\n",
    "\tbullets = [f\"- {t} (ASIN: {a})\" for t, a, _ in items]\n",
    "\theader = f\"Similar products to '{title or query}':\" if (title or query) else \"Similar products:\"\n",
    "\treturn \"\\n\".join([header, *bullets])\n",
    "\n",
    "\n",
    "@tool(\"compare_products\", return_direct=False)\n",
    "def compare_products(\n",
    "\tquery: str = \"\",\n",
    "\tproduct_a: Optional[str] = None,\n",
    "\tproduct_b: Optional[str] = None,\n",
    "\tn: Optional[int] = None,\n",
    ") -> str:\n",
    "\t\"\"\"Compare N products detected from query or provided explicitly. Inputs: query (string), optional product_a/product_b, optional n (default 2 inferred from query). Returns bullet points grouped by each product.\"\"\"\n",
    "\t# Determine desired number of items\n",
    "\ttarget_n = _parse_number_from_text(query, default=2) if n is None else int(n)\n",
    "\ttarget_n = max(2, min(6, target_n))\n",
    "\n",
    "\tcatalog = _catalog_db()\n",
    "\n",
    "\t# Collect candidate product mentions\n",
    "\tcandidates: List[str] = []\n",
    "\tfor s in [product_a, product_b]:\n",
    "\t\tif s and s.strip():\n",
    "\t\t\tcandidates.append(s.strip())\n",
    "\n",
    "\tif query:\n",
    "\t\t# Split by common comparators to extract mentions\n",
    "\t\tparts = re.split(r\"\\b(?:vs|versus|,| and | & )\\b\", query, flags=re.I)\n",
    "\t\tfor p in parts:\n",
    "\t\t\tp = re.sub(r\"\\b(compare|between|among|products?)\\b\", \" \", p, flags=re.I).strip()\n",
    "\t\t\tif p:\n",
    "\t\t\t\tcandidates.append(p)\n",
    "\n",
    "\t# Resolve to unique ASINs\n",
    "\tresolved = []  # list of (asin, title)\n",
    "\tseen_asin = set()\n",
    "\tfor cand in candidates:\n",
    "\t\tasin, title, _ = _resolve_product(cand)\n",
    "\t\tif asin and asin not in seen_asin:\n",
    "\t\t\tresolved.append((asin, title or cand))\n",
    "\t\t\tseen_asin.add(asin)\n",
    "\t\tif len(resolved) >= target_n:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# Fallback: fill from catalog by semantic search on the whole query\n",
    "\tif len(resolved) < target_n:\n",
    "\t\tresults = catalog.similarity_search_with_score(query or \"compare products\", k=target_n + 2)\n",
    "\t\tfor doc, _ in results:\n",
    "\t\t\tasin = (doc.metadata or {}).get(\"ASIN\")\n",
    "\t\t\ttitle = (doc.metadata or {}).get(\"Title\")\n",
    "\t\t\tif asin and asin not in seen_asin:\n",
    "\t\t\t\tresolved.append((asin, title or \"Unknown\"))\n",
    "\t\t\t\tseen_asin.add(asin)\n",
    "\t\t\tif len(resolved) >= target_n:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\tif len(resolved) < 2:\n",
    "\t\treturn \"I couldn't identify enough products to compare. Please specify at least two.\"\n",
    "\n",
    "\t# Retrieve contexts per product\n",
    "\tqa = _qa_db()\n",
    "\tcontexts = []  # list of (asin, title, context)\n",
    "\tfor asin, title in resolved[:target_n]:\n",
    "\t\tdocs = qa.similarity_search(query or title, k=4, filter={\"ASIN\": asin})\n",
    "\t\tctx = _join_context([d.page_content for d in docs])\n",
    "\t\tcontexts.append((asin, title, ctx))\n",
    "\n",
    "\t# If any context missing, warn gracefully\n",
    "\tif any(not ctx for _, _, ctx in contexts):\n",
    "\t\treturn (\n",
    "\t\t\t\"I couldn't retrieve enough information for all requested products to compare them reliably. \"\n",
    "\t\t\t\"Try specifying different products or updating the product data.\"\n",
    "\t\t)\n",
    "\n",
    "\t# Build dynamic prompt for N products\n",
    "\tsys = (\n",
    "\t\t\"You compare multiple products strictly using the provided contexts. \"\n",
    "\t\t\"Output bullet points grouped by each product, focusing on key features, specs, and notable differences. \"\n",
    "\t\t\"Do not fabricate details; if unknown, state it briefly. Keep it concise.\"\n",
    "\t)\n",
    "\n",
    "\thuman_lines = []\n",
    "\tfor idx, (asin, title, ctx) in enumerate(contexts, start=1):\n",
    "\t\thuman_lines.append(f\"Product {idx}: {title} (ASIN: {asin})\\nContext {idx}:\\n{ctx}\")\n",
    "\thuman = \"\\n\\n\".join(human_lines)\n",
    "\n",
    "\tprompt = ChatPromptTemplate.from_messages([\n",
    "\t\t(\"system\", sys),\n",
    "\t\t(\"human\", human),\n",
    "\t])\n",
    "\tllm = _llm()\n",
    "\tout = llm.invoke(prompt.format_messages())\n",
    "\treturn out.content\n",
    "\n",
    "\n",
    "# Optional convenience: expose a small registry for external imports\n",
    "TOOLS = [answer_product_question, recommend_products, compare_products]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "try:\n",
    "\timport tomllib as tomli  # Python 3.11+\n",
    "except Exception:  # pragma: no cover\n",
    "\ttomli = None\n",
    "\n",
    "from qa_chain import TOOLS, answer_product_question, recommend_products, compare_products\n",
    "\n",
    "def _ensure_openai_api_key() -> None:\n",
    "\t\"\"\"Ensure OPENAI_API_KEY is available via env, .env, or Streamlit secrets.\"\"\"\n",
    "\tif os.getenv(\"OPENAI_API_KEY\"):\n",
    "\t\treturn\n",
    "\t# Try default .env resolution\n",
    "\tload_dotenv()\n",
    "\tif os.getenv(\"OPENAI_API_KEY\"):\n",
    "\t\treturn\n",
    "\t# Try specific .env locations relative to app folder\n",
    "\tbase_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))\n",
    "\tfor p in [\n",
    "\t\tos.path.join(base_dir, \".env\"),\n",
    "\t\tos.path.join(os.path.dirname(base_dir), \".env\"),\n",
    "\t\tos.path.join(os.getcwd(), \".env\"),\n",
    "\t]:\n",
    "\t\ttry:\n",
    "\t\t\tif os.path.exists(p):\n",
    "\t\t\t\tload_dotenv(p)\n",
    "\t\texcept Exception:\n",
    "\t\t\tpass\n",
    "\t\tif os.getenv(\"OPENAI_API_KEY\"):\n",
    "\t\t\treturn\n",
    "\t# Try Streamlit secrets\n",
    "\tsecrets_path = os.path.join(base_dir, \".streamlit\", \"secrets.toml\")\n",
    "\tif not os.getenv(\"OPENAI_API_KEY\") and os.path.exists(secrets_path) and tomli is not None:\n",
    "\t\ttry:\n",
    "\t\t\twith open(secrets_path, \"rb\") as f:\n",
    "\t\t\t\tdata = tomli.load(f)\n",
    "\t\t\tkey = data.get(\"OPENAI_API_KEY\")\n",
    "\t\t\tif key:\n",
    "\t\t\t\tos.environ[\"OPENAI_API_KEY\"] = key\n",
    "\t\t\tmodel = data.get(\"OPENAI_MODEL\") or data.get(\"OPENAI_CHAT_MODEL\")\n",
    "\t\t\tif model and not os.getenv(\"OPENAI_MODEL\"):\n",
    "\t\t\t\tos.environ[\"OPENAI_MODEL\"] = model\n",
    "\t\texcept Exception:\n",
    "\t\t\tpass\n",
    "\n",
    "def _llm() -> ChatOpenAI:\n",
    "\t_ensure_openai_api_key()\n",
    "\tmodel = os.getenv(\"OPENAI_MODEL\", os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4o-mini\"))\n",
    "\t# Tool-calling enabled by default for OpenAI Chat models via LangChain\n",
    "\treturn ChatOpenAI(model=model, temperature=0.2)\n",
    "\n",
    "\n",
    "AGENT_SYSTEM_PROMPT = (\n",
    "\t\"You are a concise customer support AI for product Q&A Specially for beauty products.\"\n",
    "\t\"\\nGoals:\"\n",
    "\t\"\\n- You can answer general questions about beauty products.\"\n",
    "\t\"\\n- You can answer to general conversation like, 'Good morning', 'How can I help you today?'\"\n",
    "\t\"\\n- Detect the product from natural language without asking for exact names/ASINs.\"\n",
    "\t\"\\n- Answer questions strictly from retrieved product context.\"\n",
    "\t\"\\n- Suggest similar products when asked.\"\n",
    "\t\"\\n- Compare products and present bullet points grouped by each product.\"\n",
    "\t\"\\nRules:\"\n",
    "\t\"\\n- Prefer calling the provided tools when relevant (answer_product_question, recommend_products, compare_products).\"\n",
    "\t\"\\n- If context is insufficient, state uncertainty briefly.\"\n",
    "\t\"\\n- Keep answers medium and helpful. Short if user questions are encouraged.\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_agent():\n",
    "\t\"\"\"Return an agent-like runnable that calls tools as needed.\"\"\"\n",
    "\tllm = _llm().bind_tools(TOOLS)\n",
    "\tprompt = ChatPromptTemplate.from_messages([\n",
    "\t\t(\"system\", AGENT_SYSTEM_PROMPT),\n",
    "\t\t(\"human\", \"{input}\"),\n",
    "\t])\n",
    "\treturn prompt | llm\n",
    "\n",
    "\n",
    "def run_agent(query: str) -> str:\n",
    "\t\"\"\"Single-turn interaction: routes to tools and returns final text.\"\"\"\n",
    "\tagent = get_agent()\n",
    "\tresult = agent.invoke({\"input\": query})\n",
    "\n",
    "\t# If the model decided to call a tool, LangChain returns a tool-call message.\n",
    "\t# We handle one tool call round-trip for simplicity; UI can loop if needed.\n",
    "\tif hasattr(result, \"tool_calls\") and result.tool_calls:\n",
    "\t\tcall = result.tool_calls[0]\n",
    "\t\tname = call[\"name\"]\n",
    "\t\targs = call.get(\"args\", {})\n",
    "\n",
    "\t\tif name == \"answer_product_question\":\n",
    "\t\t\toutput = answer_product_question.invoke(args)\n",
    "\t\telif name == \"recommend_products\":\n",
    "\t\t\toutput = recommend_products.invoke(args)\n",
    "\t\telif name == \"compare_products\":\n",
    "\t\t\toutput = compare_products.invoke(args)\n",
    "\t\telse:\n",
    "\t\t\toutput = \"I'm not sure how to help with that.\"\n",
    "\n",
    "\t\t# Optionally, send tool output back to the model for a final polish\n",
    "\t\tllm = _llm()\n",
    "\t\tfinal = llm.invoke([\n",
    "\t\t\tSystemMessage(content=\"Rewrite the following tool result into a concise, user-friendly answer without adding new information.\"),\n",
    "\t\t\tHumanMessage(content=str(output)),\n",
    "\t\t])\n",
    "\t\treturn getattr(final, \"content\", str(output))\n",
    "\n",
    "\t# No tool call; return model's direct answer\n",
    "\treturn getattr(result, \"content\", \"I couldn't process that request.\")\n",
    "\n",
    "\n",
    "# for testing\n",
    "if __name__ == \"__main__\":\n",
    "\tquery = input(\"Enter your query: \")\n",
    "\tprint(run_agent(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb8e56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x11319ea50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11319f4d0>, root_client=<openai.OpenAI object at 0x11319c1a0>, root_async_client=<openai.AsyncOpenAI object at 0x11319f230>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bcab564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar products to 'Volumizing Powder' include:\n",
      "\n",
      "- Hair Building Fibers (ASIN: B0002ZW5UQ)\n",
      "- Root Cover Up (ASIN: B00FYSZDQ4)\n",
      "- Ceramix Xtreme Dryer (ASIN: B000ASDGK8)\n"
     ]
    }
   ],
   "source": [
    "print(run_agent(\"Show five similar products to Volumizing Powder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e4dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "556cb023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vinylux Nail Polish is a weekly polish system that includes a color coat and a top coat, designed for optimal performance. It uses patent-pending pro-light technology for durability and chip resistance, requires no base coat, and is formulated to prevent yellowing.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent('Which product related to nail polish?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c68965fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Blinc Mascara**: A no-smudge mascara that forms water-resistant tubes around lashes, providing volume and length. It resists running, smudging, clumping, and flaking, and is easy to remove without harsh products. Ideal for daily wear and workouts.\n",
      "\n",
      "**Vinylux Nail Polish**: A weekly nail polish system that combines color and top coat without needing a base coat. It features ProLight technology for enhanced durability and chip resistance, and is made to prevent yellowing. Best used with the top coat for optimal performance.\n",
      "\n",
      "**Key Differences**: Blinc Mascara is for eye makeup with easy removal, while Vinylux is a two-step nail care system that relies on light for durability.\n"
     ]
    }
   ],
   "source": [
    "print(run_agent('Compare Maskara with Nail Polish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4829fd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar products to 'Root Cover Up' include Hair Building Fibers (ASIN: B0002ZW5UQ) and Volumizing Powder (ASIN: B007P0MO2U).\n"
     ]
    }
   ],
   "source": [
    "print(run_agent('Give 4 recommendations for root cover up products'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc55d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAIProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
